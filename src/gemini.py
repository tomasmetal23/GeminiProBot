import logging
import google.generativeai as genai
from pyrogram import Client, filters
from PIL import Image

# 1. Importamos TODAS las variables necesarias desde config, incluyendo MODEL_NAME
from config import API_ID, API_HASH, GOOGLE_API_KEY, BOT_TOKEN, MODEL_NAME

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- NUEVA FUNCI√ìN PARA MANEJAR TEXTOS LARGOS ---
def send_long_message(message, text):
    """
    Divide un texto largo en trozos de 4096 caracteres y los env√≠a como mensajes separados,
    respondiendo al mensaje original del usuario.
    """
    MAX_LENGTH = 4096
    
    # Dividimos el texto en trozos del tama√±o m√°ximo permitido
    for i in range(0, len(text), MAX_LENGTH):
        chunk = text[i:i + MAX_LENGTH]
        # Usamos message.reply() para que cada parte responda al mensaje original del usuario
        message.reply(chunk, disable_web_page_preview=True)

# --- CONFIGURACI√ìN Y MODELO √öNICO DE GOOGLE ---
try:
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel(MODEL_NAME)
    logger.info(f"Modelo de Gemini cargado exitosamente: {MODEL_NAME}")
except Exception as e:
    logger.critical(f"No se pudo configurar la API de Google. Verifica tu GOOGLE_API_KEY. Error: {e}")
    exit()

# Inicializa el bot de Telegram
bot = Client("GeminiProBot", api_id=API_ID, api_hash=API_HASH, bot_token=BOT_TOKEN)

@bot.on_message(filters.command("gem"))
def handle_text_prompt(client, message):
    prompt = message.text.split(" ", 1)[1] if len(message.text.split(" ", 1)) > 1 else ""
    if not prompt:
        message.reply("Por favor, escribe un prompt despu√©s del comando. Ejemplo: `/gem ¬øCu√°l es la capital de Mongolia?`")
        return

    try:
        processing_message = message.reply("‚öôÔ∏è Procesando tu petici√≥n...")
        response = model.generate_content(prompt)
        
        # --- L√ìGICA MEJORADA PARA ENVIAR LA RESPUESTA ---
        if len(response.text) <= 4096:
            # Si el mensaje es corto, editamos el mensaje original.
            processing_message.edit_text(response.text)
        else:
            # Si el mensaje es largo, borramos "Procesando..." y usamos nuestra nueva funci√≥n.
            processing_message.delete()
            send_long_message(message, response.text)
            
    except Exception as e:
        logger.error(f"Error en handle_text_prompt: {e}")
        processing_message.edit_text(f"‚ùå Ocurri√≥ un error al procesar tu petici√≥n:\n\n`{str(e)}`")

@bot.on_message(filters.command(["imgai", "img"]) & filters.reply)
def handle_image_prompt(client, message):
    replied_message = message.reply_to_message
    if not replied_message or not replied_message.photo:
        message.reply("Por favor, usa este comando respondiendo a una imagen.")
        return

    prompt_parts = message.text.split(" ", 1)
    prompt = prompt_parts[1] if len(prompt_parts) > 1 else "Describe detalladamente lo que ves en esta imagen."
    
    try:
        processing_message = message.reply("üñºÔ∏è Analizando la imagen con tu prompt...")
        image_path = bot.download_media(replied_message.photo.file_id)
        img = Image.open(image_path)
        response = model.generate_content([prompt, img])
        
        # --- APLICAMOS LA MISMA L√ìGICA MEJORADA AQU√ç ---
        if len(response.text) <= 4096:
            processing_message.edit_text(response.text)
        else:
            processing_message.delete()
            send_long_message(message, response.text)

    except Exception as e:
        logger.error(f"Error en handle_image_prompt: {e}")
        processing_message.edit_text(f"‚ùå Ocurri√≥ un error al analizar la imagen:\n\n`{str(e)}`")


if __name__ == "__main__":
    print("ü§ñ El bot se est√° iniciando...")
    bot.run()
    print("üõë El bot se ha detenido.")